---
title: "Practical Machine Learning Project"
author: "Elena Cruz Mart√≠n"
output: html_document
---
```{r loadggplot, echo=FALSE,message=FALSE }
require(dplyr)
require(caret)
require(corrplot)
require(rattle)
require(parallel, quietly=T)
require(doParallel, quietly=T)
```

## Overview
In this exercise, we will study the results of the Unilateral Dumbbell Biceps Curl exercise performed by several different users. The objective of this study is to build a model to predict whether the user is performing the exercise well or not. For this, five different fashions for the Unilateral Dumbbell Biceps Curl have been considered:

* Class A:exactly according to the specification <- Right way to perform the exercise

* Class B:throwing the elbows to the front <- Wrong way to perform the exercise

* Class C: lifting the dumbbell only halfway <- Wrong way to perform the exercise

* Class D: lowering the dumbbell only halfway <- Wrong way to perform the exercise

* Class E: throwing the hips to the front <- Wrong way to perform the exercise

(For more information on this, read more: http://groupware.les.inf.puc-rio.br/har#ixzz3mUfOAnhN)

The dataset and dataset description is avalailable from here[http://groupware.les.inf.puc-rio.br/har]

Original paper: Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.(Read more: http://groupware.les.inf.puc-rio.br/har#ixzz3mUvZoaRB)

##Data cleaning and pre-processing
This is an important part of the work we've performed to build the machine learning model for this dataset.  After loading the dataset, a significant amount of time has been spent in evaluating the values of the different data columns, checking which ones of them had wrong or incomplete values and appliying different approaches in order to get a clean data set. 

```{r loaddata, echo=FALSE, results='hide', cache=TRUE}
data_dir <- "."
training_file_name <- "pml-training.csv"
testing_file_name <- "pml-testing.csv"

training <- read.table(file=file.path(data_dir,training_file_name),sep=",", header=TRUE)
testing <- read.table(file=file.path(data_dir,testing_file_name),sep=",", header=TRUE)
```

From the data loading we get a training set with `r ncol(training)` columns and  `r nrow(training)` different rows, while the testing set has `r nrow(testing)`, which will be used as the set of samples we need to predict after the model has been built. The name of the variable we want to predict is **classe**, that is, the manner in which the subject has performed the exercise

```{r removesummaries , echo=FALSE, results='hide', cache=TRUE}
summary_patterns <- c("max", "min","stddev", "var", "avg", "kurtosis", "skewness")
summary_columns <- unlist(sapply(summary_patterns, grep, names(training)))
training_redux <- select(training, -summary_columns)
```

A basic preliminary analysis of the data allows detecting that there are some columns that are 'summaries' of the 'raw data', that is, columns which contain the max, min, standard deviation, variance and standard deviation of the raw data. These columns have information that has been already included in the raw data information, so we will remove them in order to make our analysis simpler. We will remove the columns that contain the following patterns: `r summary_patterns`. This will allow us to remove `r length(summary_columns)` columns from the original dataset.

We also detect there are many errors in the amplitude columns. The vast majority of values in these columns are either 0, #DIV/0! or empty. Thus, we also remove these columns, as they are clearly wrong.
```{r remove_amp_cols, echo=FALSE, results='hide', cache=TRUE}
amp_yaw_pattern <- c("amplitude_")
amp_yaw_columns <- grep(amp_yaw_pattern, names(training_redux))
training_redux <- select(training_redux, -amp_yaw_columns)
```

These filtering operations allow us having a clean dataset, with no NAs.

```{r cleannotneeded,echo=FALSE, results='hide', cache=TRUE}
#Remove not needed vars 
removed_names <-names(training_redux)[1:7]
training_redux <- training_redux[,8:ncol(training_redux)]
```
Finally, we have also identified some columns that are not relevant for the analysis, which are:`r removed_names`. These columns will not be included in our analysis. 

So, `r length(amp_yaw_columns) + length(removed_names)` more columns are removed. Only `r ncol(training_redux)` remain from the original `r ncol(training)`. The selected columns for the training dataset are:

```{r showhead, echo=FALSE}
names(training_redux)
```

### Build correlation matrix and remove correlated vars
Once we have a clean dataset, the next step in our analysis is to build the correlation matrix for all the variables in the training set, and those that are highly correlated by applying the *findCorrelation* function. We have discarded those entries that have shown to be correlated amongst them. 

```{r seecorrelatedvars, cache=TRUE,fig.align='center', fig.height=7, fig.width=7,fig.pos='h',fig.cap="Correlation Matrix for the clean training Dataset",echo=FALSE}
corMatrix_entry <- select(training_redux, -classe) #We remove the classe column
corMatrix <- cor(corMatrix_entry)
correlatedCols <- findCorrelation(corMatrix, names=TRUE)
corrplot(corMatrix, method = "circle")

#Remove correlated cols 
training_redux <- training_redux[,!(names(training_redux) %in% correlatedCols)]
```

The entries with a high correlation among them are `r correlatedCols`, and our final dataset has `r ncol(training_redux)` columns.

##Model selection
We have tested several different models in order to see which one gives better results with our training sets. The models we have chosen to test are:

* PCA

* Tree model

* Random forests (on its own, not as method for pca)

(Note: In order to speed-up our processing, we have used parallelization)

```{r setcluster,echo=FALSE, results='hide',cache=TRUE}
## Turn on PP (leave at least 1 core free so your machine is still usable when doing the calc)
cluster <- makeCluster(detectCores() - 1)
registerDoParallel(cluster)
```

###Principal components model
We run the principal components model with the whole remaining training dataset. 
```{r pcamodel, echo=FALSE, cache=TRUE, results='hide', warning=FALSE}
preProc <- train(classe ~ ., data = training_redux, preProcess="pca")
```

The results we obtain for this analysis are:
```{r pcamodelshow, echo=FALSE, cache=TRUE}
print(preProc$finalModel)
```

###Tree model
As we have done for the principal components model, we build and run the tree model with the whole dataset:
```{r treemodel, cache=TRUE,fig.align='center', fig.height=5, fig.width=5,fig.pos='h',fig.cap="Tree model",echo=FALSE}
modFit_tree <- train(classe~., data = training_redux, method="rpart")
print(modFit_tree$finalModel)
fancyRpartPlot(modFit_tree$finalModel, main="Final model for the Tree model")
```


###Random forest model
Finally, we also test a random forest model with the whole dataset:
```{r rfmodel, cache=TRUE, echo=FALSE, results='hide'}
modFit_rf <- train(classe~., data = training_redux, method="rf")
```

```{r rfmodelshow, cache=TRUE, echo=FALSE}
print(modFit_rf$finalModel)
```

```{r stopcluster,echo=FALSE, results='hide', cache=TRUE}
stopCluster(cluster)
```


##Conclusions and results
From the results of the different models, we can see that the random forest model has the lowest OOB estimate of error rate (see final model OOB estimate of error rate above, as well as the confusion matrix). Thus, this is the model we will select for our analysis (and this also serves as cross-validation).

The result dataset we obtain for the testing set wit the rf model is the following:
```{r showresults, echo=FALSE}
answers_rf <- predict(modFit_rf,testing)
answers_rf
```

NOTE: To run this Rmd file, you need to have in its same directory the pml-training.csv and pml-testing.csv files.





